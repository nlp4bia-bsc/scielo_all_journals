{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store all journals\n",
    "all_journals = []\n",
    "\n",
    "def get_all_categories():\n",
    "    # Base URL where all categories are listed\n",
    "    base_url = \"https://scielo.org/es/revistas/listar-por-tema\"\n",
    "    response = requests.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve categories.\")\n",
    "        return {}\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the menu or list containing categories\n",
    "    categories = {}\n",
    "    \n",
    "    # Categories are inside select eelement\n",
    "    category_options = soup.select(\"#subject_area option\")\n",
    "    \n",
    "    for option in category_options:\n",
    "        category_name = option.text.strip()\n",
    "        category_url = option.get(\"value\")\n",
    "        \n",
    "        # add valid category URLs\n",
    "        if category_url and \"listar-por-tema\" in category_url:\n",
    "            categories[category_name] = category_url\n",
    "    \n",
    "    return categories\n",
    "\n",
    "def get_journals_from_letter_page(url, category_name):\n",
    "    # Request the page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve journals from {url}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Extract journal entries\n",
    "    journal_rows = soup.select('#journalsTable tbody tr')\n",
    "    \n",
    "    # Iterate over each row to get journal details\n",
    "    for row in journal_rows:\n",
    "        # Check if it is a separator row by letter, skip if true\n",
    "        if \"separator-by-letter\" in row.get(\"class\", []):\n",
    "            continue\n",
    "        \n",
    "        # Extract journal name and URL\n",
    "        journal_link = row.find(\"a\")\n",
    "        if journal_link:\n",
    "            journal_name = journal_link.text.strip()\n",
    "            journal_url = journal_link['href']\n",
    "            # Check if the journal appears inactive by checking 'disabled' class\n",
    "            journal_type = \"Inactive\" if 'disabled' in journal_link.get(\"class\", []) else \"Active\"\n",
    "            # Append the data\n",
    "            all_journals.append({\n",
    "                \"journal_name\": journal_name,\n",
    "                \"category\": category_name,\n",
    "                \"type\": journal_type,\n",
    "                \"URL\": journal_url\n",
    "            })\n",
    "\n",
    "def get_all_journals_from_category(category_url, category_name):\n",
    "    # Request the category page to get the alphabetic filter buttons\n",
    "    response = requests.get(category_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve category page {category_url}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract letter buttons\n",
    "    letter_buttons = soup.select('.btn-group-vertical .btn')\n",
    "    letters = [btn.text.strip() for btn in letter_buttons if btn.text.strip()]\n",
    "\n",
    "    # Iterate through each letter and scrape the journals\n",
    "    for letter in letters:\n",
    "        # Generate the URL for the specific letter\n",
    "        letter_url = f\"{category_url}?letter={letter}\"\n",
    "        print(f\"Scraping {category_name} - Letter {letter}\")\n",
    "        get_journals_from_letter_page(letter_url, category_name)\n",
    "        # To prevent overwhelming the server, add a small delay\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dynamically extract categories\n",
    "categories = get_all_categories()\n",
    "\n",
    "# iterate over categories\n",
    "for category, category_url in categories.items():\n",
    "    print(f\"Scraping category: {category}\")\n",
    "    get_all_journals_from_category(category_url, category)\n",
    "\n",
    "# Create df\n",
    "df = pd.DataFrame(all_journals)\n",
    "\n",
    "# Save df to csv\n",
    "output_path = \"./scielo_all_journals.csv\" # TODO: add your output path\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSTPROCESSING: some journals are labelled as being in different categories (Example: 'Todas las categorías', 'Humanidades' and 'Ciencias Biológicas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load output_path csv file \n",
    "input_path = \"./scielo_all_journals.csv\" # TODO: this is the output path of the previous folder\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Create a set to hold all unique category names for generating column headers\n",
    "unique_categories = set(df['category'].unique())\n",
    "\n",
    "# Format category names to be suitable for column names\n",
    "category_columns = {\n",
    "    category: f\"category_{category.lower().replace(' ', '_').replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('ñ', 'n').replace('ç', 'c')}\"\n",
    "    for category in unique_categories\n",
    "}\n",
    "\n",
    "# Create a dic for data\n",
    "journal_dict = {}\n",
    "\n",
    "# Iterate over each row to organize journals by name\n",
    "for index, row in df.iterrows():\n",
    "    journal_name = row['journal_name']\n",
    "    category = row['category']\n",
    "    journal_type = row['type']\n",
    "    journal_url = row['URL']\n",
    "    \n",
    "    # If the journal is already in the dictionary, just add the new category name\n",
    "    if journal_name in journal_dict:\n",
    "        journal_dict[journal_name][category_columns[category]] = category\n",
    "    else:\n",
    "        # Create a new entry for this journal, initializing all category columns to 0\n",
    "        journal_data = {\n",
    "            \"journal_name\": journal_name,\n",
    "            \"type\": journal_type,\n",
    "            \"URL\": journal_url,\n",
    "            **{col_name: 0 for col_name in category_columns.values()}\n",
    "        }\n",
    "        # Set the relevant category column to the category name\n",
    "        journal_data[category_columns[category]] = category\n",
    "        journal_dict[journal_name] = journal_data\n",
    "\n",
    "# Prepare new rows for the cleaned df\n",
    "cleaned_data = list(journal_dict.values())\n",
    "\n",
    "# Create a new df from the cleaned data\n",
    "cleaned_df = pd.DataFrame(cleaned_data)\n",
    "\n",
    "# Save cleaned df to a new csv file\n",
    "output_path = \"./scielo_cleaned_journals.csv\" # TODO: new output path with the filterd/cleaned data\n",
    "cleaned_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
